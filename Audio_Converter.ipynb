{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Note: Epoch 0, Test 0\n",
      "Phase 1\n",
      "Phase 2\n",
      "Phase 3\n",
      "Note: Epoch 0, Test 1\n",
      "Phase 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as tdata\n",
    "import os \n",
    "from torch import dtype\n",
    "from google.colab import drive\n",
    "import skimage\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "\n",
    "def rescale_resnet(model):\n",
    "    first_conv_layer = [nn.Conv2d(1, 3, 4, stride=1, padding=1, dilation=1, groups=1, bias=True)]\n",
    "    first_conv_layer.extend([model.features])  \n",
    "    model.features= nn.Sequential(*first_conv_layer )\n",
    "    return model\n",
    "    \n",
    "\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "model_conv.fc = torch.nn.Linear(model_conv.fc.in_features, 9)#says it doesnt recognize model_conv.classifier\n",
    "model_conv = nn.Sequential(nn.Conv2d(1, 3, 4, stride=1, padding=1, dilation=1, groups=1, bias=True),model_conv)\n",
    "model_conv_2= torchvision.models.resnet18(pretrained=True)\n",
    "model_conv_2.fc = torch.nn.Linear(model_conv_2.fc.in_features,3)\n",
    "for param in model_conv_2.parameters():\n",
    "    param.requires_grad = False\n",
    "model_conv_2 = nn.Sequential(nn.Conv2d(1, 3, 4, stride=1, padding=1, dilation=1, groups=1, bias=True),model_conv_2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "model_conv_2 = model_conv_2.to(device)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#we need a custom grayscale resnet to train\n",
    "class GrayScale_Resnet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GrayScale_Resnet,self).__init__()\n",
    "\n",
    "\n",
    "#hyperparameters for CNNs\n",
    "btch_sz = 64\n",
    "learning_rate = 0.3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epoch = 12\n",
    "MODEL_NOTE_PATH = '/content/drive/My Drive/data/Current-Model-Note/model_one.txt'\n",
    "MODEL_ACC_PATH = '/content/drive/My Drive/data/Current-Model-Acc/model_two.txt'\n",
    "\n",
    "#NOTE TO SELF: REVISE LABEL TXT FILE BECAUSE THE FLAT/SHARP/NATS ARE MISPLACED (ALSO DEAL WITH DYNAMIC LATER, \n",
    "#MORE PICTURES NEEDED)\n",
    "\n",
    "def process_labels_note(label_set):\n",
    "    for i in open(label_set).readlines():\n",
    "        strn = i\n",
    "        #print(strn)\n",
    "        #print(\"{},{}\".format(ord(strn[0]), ord('A')))\n",
    "        char_arr= strn.split(',')\n",
    "        #for j in char_arr:\n",
    "            #print(\"{} + {}\".format(\"CHAR ELEM:\", j))\n",
    "        tensor_conv = [0 for i in range(7)]\n",
    "        tensor_dynamic = [0 for i in range(2)]\n",
    "        tensor_conv[ord(char_arr[0]) - ord('A')]=1\n",
    "        #print(\"{}:{}\".format(\"STRN 1\",strn[1]))\n",
    "\n",
    "\n",
    "        if(char_arr[1]=='F'):\n",
    "            tensor_dynamic[0] = 1\n",
    "        elif(char_arr[1]=='P'):\n",
    "            tensor_dynamic[1] = 1\n",
    "        vectorized_note = []\n",
    "        vectorized_note.append(tensor_conv + tensor_dynamic)\n",
    "    \n",
    "        yield vectorized_note\n",
    "def process_labels_acc(label_set):\n",
    "    for i in open(label_set).readlines():\n",
    "        tensor_acc = [0 for k in range(3)]\n",
    "        #print(i)\n",
    "        i=i.replace('\\n','')\n",
    "        if(i == 'FLAT'):\n",
    "            #print(0)\n",
    "            tensor_acc[0] = 1\n",
    "        elif(i == 'NAT'):\n",
    "            #print(1)\n",
    "            tensor_acc[1] = 1\n",
    "        elif(i == 'SHARP'):\n",
    "            #print(2)\n",
    "            tensor_acc[2] = 1\n",
    "        \n",
    "        yield tensor_acc\n",
    "\n",
    "class Gesture_Dataset_Note_and_Volume(tdata.Dataset):\n",
    "    @staticmethod\n",
    "    def run():\n",
    "        pass\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        for lbl in range(len(self.labels)):\n",
    "            self.labels[lbl] = torch.tensor(self.labels[lbl]).squeeze(0)\n",
    "        #self.images = torch.tensor(self.images)\n",
    "        #self.labels= torch.tensor(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.images[idx],self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "class Gesture_Dataset_Accidental(tdata.Dataset):\n",
    "    def __init__(self,images,labels,transform=None):\n",
    "        self.images=images\n",
    "        self.labels=labels\n",
    "        for lbl in range(len(self.labels)):\n",
    "            self.labels[lbl] = torch.tensor(self.labels[lbl]).squeeze(0)\n",
    "        #self.images= torch.tensor(self.images)\n",
    "        #self.labels= torch.tensor(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.images[idx],self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "#pitch will be based on the angle at which the left hand is oriented\n",
    "#volume will be based on the fingers' separation from each other\n",
    "\n",
    "class CNN_Base_Note_and_Volume(nn.Module):\n",
    "    #assume \n",
    "    def __init__(self, input_size_x, input_size_y):\n",
    "        super(CNN_Base_Note_and_Volume, self).__init__()\n",
    "        self.conv2d_1 = torch.nn.Conv2d(1,3,12)\n",
    "        self.relu_1 = torch.nn.ReLU()\n",
    "        self.mp_1 = torch.nn.MaxPool2d(5)\n",
    "        self.conv2d_2 = torch.nn.Conv2d(3,6,9)\n",
    "        self.relu_2 = torch.nn.ReLU()\n",
    "        self.mp_2 = torch.nn.MaxPool2d(5)\n",
    "        self.conv2d_3 = torch.nn.Conv2d(6,9,6)\n",
    "        self.relu_3 = torch.nn.ReLU()\n",
    "        self.mp_3 = torch.nn.MaxPool2d(3)\n",
    "        self.conv2d_4=torch.nn.Conv2d(9,12,4)\n",
    "        self.relu_4 = torch.nn.ReLU()\n",
    "        self.mp_4 = torch.nn.MaxPool2d(2)\n",
    "        self.conv2d_5 = torch.nn.Conv2d(12,14,3)\n",
    "        self.relu_5 = torch.nn.ReLU()\n",
    "        self.mp_5 = torch.nn.MaxPool2d(2)\n",
    "        self.ftn_1 = torch.nn.Linear(720, 300)\n",
    "        self.ftn_2= torch.nn.Linear(300, 100)\n",
    "        self.ftn_3=torch.nn.Linear(100, 9)\n",
    "        #fully connected layer\n",
    "    def forward(self, input_note_image):\n",
    "        one_conv = self.conv2d_1(input_note_image)\n",
    "        one_rel = self.relu_1(one_conv)\n",
    "        one_mp = self.mp_1(one_rel)\n",
    "        two_conv = self.conv2d_2(one_mp)\n",
    "        two_rel = self.relu_2(two_conv)\n",
    "        two_mp = self.mp_2(two_rel)\n",
    "        three_conv = self.conv2d_3(two_mp)\n",
    "        three_rel = self.relu_3(three_conv)\n",
    "        three_mp = self.mp_3(three_rel)\n",
    "        four_conv = self.conv2d_4(three_mp)\n",
    "        four_rel = self.relu_4(four_conv)\n",
    "        inp_linear = self.ftn_1(three_mp)\n",
    "        inp_two = self.ftn_2(inp_linear)\n",
    "        inp_three = self.ftn_3(inp_two)\n",
    "        return inp_three\n",
    "        #print(input_image.shape)\n",
    "        \n",
    "#accidental will be based on the extending of some finger\n",
    "class CNN_Base_Accidental(nn.Module):\n",
    "    #consider using less conv/relu layers for accidental, may lead to overfitting\n",
    "    def __init__(self, input_x, input_y):\n",
    "        super(CNN_Base_Accidental, self).__init__()\n",
    "        self.conv2d_1 = torch.nn.Conv2d(1,3,12)\n",
    "        self.relu_1 = torch.nn.ReLU()\n",
    "        self.mp_1 = torch.nn.MaxPool2d(5)\n",
    "        self.conv2d_2 = torch.nn.Conv2d(3,6,11)\n",
    "        self.relu_2 = torch.nn.ReLU()\n",
    "        self.mp_2 = torch.nn.MaxPool2d(5)\n",
    "        self.conv2d_3 = torch.nn.Conv2d(6,9,9)\n",
    "        self.relu_3 = torch.nn.ReLU()\n",
    "        self.mp_3 = torch.nn.MaxPool2d(3)\n",
    "        self.conv2d_4=torch.nn.Conv2d(9,12,5)\n",
    "        self.relu_4 = torch.nn.ReLU()\n",
    "        self.mp_4 = torch.nn.MaxPool2d(2)\n",
    "        self.conv2d_5 = torch.nn.Conv2d(12,14,3)\n",
    "        self.relu_5 = torch.nn.ReLU()\n",
    "        self.mp_5 = torch.nn.MaxPool2d(2)\n",
    "        self.ftn_1 = torch.nn.Linear(720,300)\n",
    "        self.ftn_2= torch.nn.Linear(300,100)\n",
    "        self.ftn_3=torch.nn.Linear(100,11)\n",
    "    def forward(self, input_acc_image):\n",
    "        one_conv = self.conv2d_1(input_acc_image)\n",
    "        one_rel = self.relu_1(one_conv)\n",
    "        one_mp = self.mp_1(one_rel)\n",
    "        two_conv = self.conv2d_2(one_mp)\n",
    "        two_rel = self.relu_2(two_conv)\n",
    "        two_mp = self.mp_2(two_rel)\n",
    "        three_conv = self.conv2d_3(two_mp)\n",
    "        three_rel = self.relu_3(three_conv)\n",
    "        three_mp = self.mp_3(three_rel)\n",
    "        four_conv = self.conv2d_4(three_mp)\n",
    "        four_rel = self.relu_4(four_conv)\n",
    "        inp_linear = self.ftn_1(three_mp)\n",
    "        inp_two = self.ftn_2(inp_linear)\n",
    "        inp_three = self.ftn_3(inp_two)\n",
    "        return inp_three\n",
    "\n",
    "train_set_note = os.listdir(\"/content/drive/My Drive/data/Image_Training_Note\")\n",
    "test_set_note = os.listdir(\"/content/drive/My Drive/data/Image_Testing_Note\")\n",
    "train_set_accidental = os.listdir('/content/drive/My Drive/data/Image_Training_Accidental')\n",
    "test_set_accidental = os.listdir('/content/drive/My Drive/data/Image_Testing_Accidental')\n",
    "label_set = os.path.join(\"/content/drive/My Drive/data/Labels/tonal_labels_train_and_test_note.txt\")\n",
    "label_set_two = os.path.join(\"/content/drive/My Drive/data/Labels/tonal_labels_train_and_test_accidental.txt\")\n",
    "image_train= []\n",
    "image_acc = []\n",
    "image_test=[]\n",
    "labels_note = []\n",
    "labels_accidental = []\n",
    "\n",
    "Gesture_Dataset_Note_and_Volume.run()\n",
    "'''\n",
    "transform_img = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "'''\n",
    "print(\"NOTE/DYNAMIC IMAGE TRAIN SET\")\n",
    "for i in range(1,len(train_set_note)):\n",
    "    print(train_set_note[i])\n",
    "    if(train_set_note[i] == \".DS_Store\"):\n",
    "      continue\n",
    "    im = Image.open(os.path.join('/content/drive/My Drive/data/Image_Training_Note/', train_set_note[i]))\n",
    "    image_train.append(transforms.ToTensor()(im))\n",
    "print(\"ACCIDENTAL IMAGE TRAIN SET\")\n",
    "for i in range(1,len(train_set_accidental)):\n",
    "    print(train_set_accidental[i])\n",
    "    if(train_set_accidental[i] == \".DS_Store\"):\n",
    "      continue\n",
    "    im = Image.open(os.path.join('/content/drive/My Drive/data/Image_Training_Accidental/', train_set_accidental[i]))\n",
    "    image_acc.append(transforms.ToTensor()(im))\n",
    "    \n",
    "\n",
    "idx = 0\n",
    "note_tensor = []\n",
    "acc_tensor = []\n",
    "for ret in process_labels_note(label_set):\n",
    "    note_tensor.append(ret)\n",
    "for ret in process_labels_acc(label_set_two):\n",
    "    acc_tensor.append(ret)\n",
    "'''\n",
    "for labels in note_tensor:\n",
    "    print(\"{}: {}\".format(\"NOTE TENSOR\", labels))\n",
    "for labels in acc_tensor:\n",
    "    print(\"{}: {}\".format(\"ACCIDENTAL TENSOR\", labels))\n",
    "for labels in dyn_tensor:\n",
    "    print(\"{}: {}\".format(\"DYNAMIC TENSOR\", labels))\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "gest_data = Gesture_Dataset_Note_and_Volume(image_train, note_tensor)\n",
    "acc_data = Gesture_Dataset_Accidental(image_acc, acc_tensor,transform=transforms.ToTensor())\n",
    "note_loader = torch.utils.data.DataLoader(gest_data, batch_size = btch_sz, shuffle=True, num_workers = 3)\n",
    "acc_loader = torch.utils.data.DataLoader(acc_data, batch_size = btch_sz,shuffle=True, num_workers = 3)\n",
    "cnn_note = CNN_Base_Note_and_Volume(540,720).to(device)\n",
    "cnn_accidental = CNN_Base_Accidental(540,720).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_note = torch.optim.SGD(cnn_note.parameters(), lr=learning_rate) \n",
    "optimizer_acc = torch.optim.SGD(cnn_accidental.parameters(),lr=learning_rate)\n",
    "optimizer_conv_note = torch.optim.SGD(model_conv.parameters(),lr=learning_rate)\n",
    "optimizer_conv_acc = torch.optim.SGD(model_conv_2.parameters(),lr=learning_rate)\n",
    "\n",
    "    \n",
    "#OPTIMIZATION PROCESS\n",
    "\n",
    "\n",
    "for n_e in range(epoch):\n",
    "    for idx, (i,j) in enumerate(note_loader):\n",
    "        print(\"Note: Epoch {}, Test {}\".format(n_e, idx))\n",
    "        i=i.to(device)\n",
    "        #print(i.shape)\n",
    "        j=j.to(device)\n",
    "        #print(j.shape)\n",
    "        print(\"Phase 1\")\n",
    "        if torch.cuda.is_available():\n",
    "          op = model_conv(i)\n",
    "          j = j.float()\n",
    "          ls = criterion(op,j)\n",
    "          print(\"Phase 2\")\n",
    "          optimizer_conv_note.zero_grad()\n",
    "          ls.backward()\n",
    "          optimizer_conv_note.step()\n",
    "          print(\"Phase 3\")\n",
    "print(\"NOTE (AND DYNAMIC) TRAINING COMPLETE!\")\n",
    "for n_e in range(epoch):\n",
    "    for idx, (i,j) in enumerate(acc_loader):\n",
    "        print(\"Accidental: Epoch {}, Test {}\".format(n_e, idx))\n",
    "        i=i.to(device)\n",
    "        j=j.to(device)\n",
    "        j = j.float()\n",
    "        print(\"Phase 1\")\n",
    "        if torch.cuda.is_available():\n",
    "          op = model_conv_2(i)\n",
    "          ls = criterion(op,j)\n",
    "          print(\"Phase 2\")\n",
    "          optimizer_conv_acc.zero_grad()\n",
    "          ls.backward()\n",
    "          optimizer_conv_acc.step()\n",
    "          print(\"Phase 3\")\n",
    "print(\"ACCIDENTAL TRAINING COMPLETE!\")\n",
    "\n",
    "\n",
    "torch.save(model_conv.state_dict(), MODEL_NOTE_PATH)\n",
    "torch.save(model_conv_2.state_dict(), MODEL_ACC_PATH)\n",
    "        \n",
    "print(\"SAVED!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as tdata\n",
    "import os \n",
    "from torch import dtype\n",
    "import pyaudio\n",
    "import skimage\n",
    "import pyaudio\n",
    "from PIL import Image\n",
    "import itertools\n",
    "\n",
    "MODEL_NOTE_PATH = '/content/drive/My Drive/data/Current-Model-Note/model_one.txt'\n",
    "MODEL_ACC_PATH = '/content/drive/My Drive/data/Current-Model-Acc/model_two.txt'\n",
    "m1 = torch.load(MODEL_NOTE_PATH)\n",
    "m2 = torch.load(MODEL_ACC_PATH)\n",
    "print(m1)\n",
    "print(m2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "def take_photo(filename, quality=1):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      //google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = 1080\n",
    "      canvas.height = 720\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)\n",
    "  return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import clear_output\n",
    "idx = 0\n",
    "def run():\n",
    "  filename = ''\n",
    "  while(filename == ''):\n",
    "    try:\n",
    "      global idx\n",
    "      filename = take_photo(\"image_\"+str(idx)+\".jpeg\")\n",
    "      if(filename != ''):\n",
    "        idx += 1\n",
    "      print('Saved to {}'.format(filename))\n",
    "      #Show the image which was just taken.\n",
    "      display(Image(filename))\n",
    "      clear_output()\n",
    "    except Exception as err:\n",
    "      # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "      # grant the page permission to access it.\n",
    "      print(str(err))\n",
    "\n",
    "def run_glob():\n",
    "    while(True):\n",
    "      run()\n",
    "\n",
    "try:\n",
    "  run_glob()\n",
    "except Exception as err:\n",
    "  print(\"TERMINATING STREAM...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
