{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'gpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fd77d164479c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpu'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'gpu'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as tdata\n",
    "import os \n",
    "from torch import dtype\n",
    "import pyaudio\n",
    "import skimage\n",
    "import pyaudio\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def rescale_resnet(model):\n",
    "    first_conv_layer = [nn.Conv2d(1, 3, 4, stride=1, padding=1, dilation=1, groups=1, bias=True)]\n",
    "    first_conv_layer.extend([model.features])  \n",
    "    model.features= nn.Sequential(*first_conv_layer )\n",
    "    return model\n",
    "    \n",
    "\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "model_conv.fc = torch.nn.Linear(model_conv.fc.in_features, 9)#says it doesnt recognize model_conv.classifier\n",
    "model_conv = nn.Sequential(nn.Conv2d(1, 3, 4, stride=1, padding=1, dilation=1, groups=1, bias=True),model_conv)\n",
    "model_conv_2= torchvision.models.resnet18(pretrained=True)\n",
    "model_conv_2.fc = torch.nn.Linear(model_conv_2.fc.in_features,2)\n",
    "for param in model_conv_2.parameters():\n",
    "    param.requires_grad = False\n",
    "model_conv_2 = nn.Sequential(nn.Conv2d(1, 3, 4, stride=1, padding=1, dilation=1, groups=1, bias=True),model_conv_2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "model_conv_2 = model_conv_2.to(device)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#we need a custom grayscale resnet to train\n",
    "class GrayScale_Resnet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GrayScale_Resnet,self).__init__()\n",
    "\n",
    "\n",
    "#hyperparameters for CNNs\n",
    "btch_sz = 64\n",
    "learning_rate = 0.3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epoch = 25\n",
    "MODEL_NOTE_PATH = './data/Current-Model-Note'\n",
    "MODEL_ACC_PATH = './data/Current-Model-Acc'\n",
    "\n",
    "#NOTE TO SELF: REVISE LABEL TXT FILE BECAUSE THE FLAT/SHARP/NATS ARE MISPLACED (ALSO DEAL WITH DYNAMIC LATER, \n",
    "#MORE PICTURES NEEDED)\n",
    "\n",
    "def process_labels_note(label_set):\n",
    "    for i in open(label_set).readlines():\n",
    "        strn = i\n",
    "        #print(strn)\n",
    "        #print(\"{},{}\".format(ord(strn[0]), ord('A')))\n",
    "        char_arr= strn.split(',')\n",
    "        #for j in char_arr:\n",
    "            #print(\"{} + {}\".format(\"CHAR ELEM:\", j))\n",
    "        tensor_conv = [0 for i in range(7)]\n",
    "        tensor_dynamic = [0 for i in range(2)]\n",
    "        tensor_conv[ord(char_arr[0]) - ord('A')]=1\n",
    "        #print(\"{}:{}\".format(\"STRN 1\",strn[1]))\n",
    "\n",
    "\n",
    "        if(char_arr[1]=='F'):\n",
    "            tensor_dynamic[0] = 1\n",
    "        elif(char_arr[1]=='P'):\n",
    "            tensor_dynamic[1] = 1\n",
    "        vectorized_note = []\n",
    "        vectorized_note.append(tensor_conv + tensor_dynamic)\n",
    "    \n",
    "        yield vectorized_note\n",
    "def process_labels_acc(label_set):\n",
    "    for i in open(label_set).readlines():\n",
    "        tensor_acc = [0 for k in range(3)]\n",
    "        #print(i)\n",
    "        i=i.replace('\\n','')\n",
    "        if(i == 'FLAT'):\n",
    "            #print(0)\n",
    "            tensor_acc[0] = 1\n",
    "        elif(i == 'NAT'):\n",
    "            #print(1)\n",
    "            tensor_acc[1] = 1\n",
    "        elif(i == 'SHARP'):\n",
    "            #print(2)\n",
    "            tensor_acc[2] = 1\n",
    "        \n",
    "        yield tensor_acc\n",
    "\n",
    "class Gesture_Dataset_Note_and_Volume(tdata.Dataset):\n",
    "    @staticmethod\n",
    "    def run():\n",
    "        pass\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        for lbl in range(len(self.labels)):\n",
    "            self.labels[lbl] = torch.tensor(self.labels[lbl]).squeeze(0)\n",
    "        #self.images = torch.tensor(self.images)\n",
    "        #self.labels= torch.tensor(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.images[idx],self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "class Gesture_Dataset_Accidental(tdata.Dataset):\n",
    "    def __init__(self,images,labels,transform=None):\n",
    "        self.images=images\n",
    "        self.labels=labels\n",
    "        for lbl in range(len(self.labels)):\n",
    "            self.labels[lbl] = torch.tensor(self.labels[lbl]).squeeze(0)\n",
    "        #self.images= torch.tensor(self.images)\n",
    "        #self.labels= torch.tensor(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.images[idx],self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "#pitch will be based on the angle at which the left hand is oriented\n",
    "#volume will be based on the fingers' separation from each other\n",
    "\n",
    "class CNN_Base_Note_and_Volume(nn.Module):\n",
    "    #assume \n",
    "    def __init__(self, input_size_x, input_size_y):\n",
    "        super(CNN_Base_Note_and_Volume, self).__init__()\n",
    "        self.conv2d_1 = torch.nn.Conv2d(1,3,12)\n",
    "        self.relu_1 = torch.nn.ReLU()\n",
    "        self.mp_1 = torch.nn.MaxPool2d(5)\n",
    "        self.conv2d_2 = torch.nn.Conv2d(3,6,9)\n",
    "        self.relu_2 = torch.nn.ReLU()\n",
    "        self.mp_2 = torch.nn.MaxPool2d(5)\n",
    "        self.conv2d_3 = torch.nn.Conv2d(6,9,6)\n",
    "        self.relu_3 = torch.nn.ReLU()\n",
    "        self.mp_3 = torch.nn.MaxPool2d(3)\n",
    "        self.conv2d_4=torch.nn.Conv2d(9,12,4)\n",
    "        self.relu_4 = torch.nn.ReLU()\n",
    "        self.mp_4 = torch.nn.MaxPool2d(2)\n",
    "        self.conv2d_5 = torch.nn.Conv2d(12,14,3)\n",
    "        self.relu_5 = torch.nn.ReLU()\n",
    "        self.mp_5 = torch.nn.MaxPool2d(2)\n",
    "        self.ftn_1 = torch.nn.Linear(720, 300)\n",
    "        self.ftn_2= torch.nn.Linear(300, 100)\n",
    "        self.ftn_3=torch.nn.Linear(100, 9)\n",
    "        #fully connected layer\n",
    "    def forward(self, input_note_image):\n",
    "        one_conv = self.conv2d_1(input_note_image)\n",
    "        one_rel = self.relu_1(one_conv)\n",
    "        one_mp = self.mp_1(one_rel)\n",
    "        two_conv = self.conv2d_2(one_mp)\n",
    "        two_rel = self.relu_2(two_conv)\n",
    "        two_mp = self.mp_2(two_rel)\n",
    "        three_conv = self.conv2d_3(two_mp)\n",
    "        three_rel = self.relu_3(three_conv)\n",
    "        three_mp = self.mp_3(three_rel)\n",
    "        four_conv = self.conv2d_4(three_mp)\n",
    "        four_rel = self.relu_4(four_conv)\n",
    "        inp_linear = self.ftn_1(three_mp)\n",
    "        inp_two = self.ftn_2(inp_linear)\n",
    "        inp_three = self.ftn_3(inp_two)\n",
    "        return inp_three\n",
    "        #print(input_image.shape)\n",
    "        \n",
    "#accidental will be based on the extending of some finger\n",
    "class CNN_Base_Accidental(nn.Module):\n",
    "    #consider using less conv/relu layers for accidental, may lead to overfitting\n",
    "    def __init__(self, input_x, input_y):\n",
    "        super(CNN_Base_Accidental, self).__init__()\n",
    "        self.conv2d_1 = torch.nn.Conv2d(1,3,12)\n",
    "        self.relu_1 = torch.nn.ReLU()\n",
    "        self.mp_1 = torch.nn.MaxPool2d(5)\n",
    "        self.conv2d_2 = torch.nn.Conv2d(3,6,11)\n",
    "        self.relu_2 = torch.nn.ReLU()\n",
    "        self.mp_2 = torch.nn.MaxPool2d(5)\n",
    "        self.conv2d_3 = torch.nn.Conv2d(6,9,9)\n",
    "        self.relu_3 = torch.nn.ReLU()\n",
    "        self.mp_3 = torch.nn.MaxPool2d(3)\n",
    "        self.conv2d_4=torch.nn.Conv2d(9,12,5)\n",
    "        self.relu_4 = torch.nn.ReLU()\n",
    "        self.mp_4 = torch.nn.MaxPool2d(2)\n",
    "        self.conv2d_5 = torch.nn.Conv2d(12,14,3)\n",
    "        self.relu_5 = torch.nn.ReLU()\n",
    "        self.mp_5 = torch.nn.MaxPool2d(2)\n",
    "        self.ftn_1 = torch.nn.Linear(720,300)\n",
    "        self.ftn_2= torch.nn.Linear(300,100)\n",
    "        self.ftn_3=torch.nn.Linear(100,11)\n",
    "    def forward(self, input_acc_image):\n",
    "        one_conv = self.conv2d_1(input_acc_image)\n",
    "        one_rel = self.relu_1(one_conv)\n",
    "        one_mp = self.mp_1(one_rel)\n",
    "        two_conv = self.conv2d_2(one_mp)\n",
    "        two_rel = self.relu_2(two_conv)\n",
    "        two_mp = self.mp_2(two_rel)\n",
    "        three_conv = self.conv2d_3(two_mp)\n",
    "        three_rel = self.relu_3(three_conv)\n",
    "        three_mp = self.mp_3(three_rel)\n",
    "        four_conv = self.conv2d_4(three_mp)\n",
    "        four_rel = self.relu_4(four_conv)\n",
    "        inp_linear = self.ftn_1(three_mp)\n",
    "        inp_two = self.ftn_2(inp_linear)\n",
    "        inp_three = self.ftn_3(inp_two)\n",
    "        return inp_three\n",
    "\n",
    "train_set_note = os.listdir(\"./data/Image_Training_Note\")\n",
    "test_set_note = os.listdir(\"./data/Image_Testing_Note\")\n",
    "train_set_accidental = os.listdir('./data/Image_Training_Accidental')\n",
    "test_set_accidental = os.listdir('./data/Image_Testing_Accidental')\n",
    "label_set = os.path.join(\"./data/Labels/tonal_labels_train_and_test_note.txt\")\n",
    "label_set_two = os.path.join(\"./data/Labels/tonal_labels_train_and_test_accidental.txt\")\n",
    "image_train= []\n",
    "image_test=[]\n",
    "labels_note = []\n",
    "labels_accidental = []\n",
    "\n",
    "Gesture_Dataset_Note_and_Volume.run()\n",
    "'''\n",
    "transform_img = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "'''\n",
    "for i in range(1,len(train_set_note)):\n",
    "    im = Image.open(os.path.join('./data/Image_Training_Note/', train_set_note[i]))\n",
    "    image_train.append(transforms.ToTensor()(im))\n",
    "    \n",
    "\n",
    "idx = 0\n",
    "note_tensor = []\n",
    "acc_tensor = []\n",
    "for ret in process_labels_note(label_set):\n",
    "    note_tensor.append(ret)\n",
    "for ret in process_labels_acc(label_set_two):\n",
    "    acc_tensor.append(ret)\n",
    "'''\n",
    "for labels in note_tensor:\n",
    "    print(\"{}: {}\".format(\"NOTE TENSOR\", labels))\n",
    "for labels in acc_tensor:\n",
    "    print(\"{}: {}\".format(\"ACCIDENTAL TENSOR\", labels))\n",
    "for labels in dyn_tensor:\n",
    "    print(\"{}: {}\".format(\"DYNAMIC TENSOR\", labels))\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "gest_data = Gesture_Dataset_Note_and_Volume(image_train, note_tensor)\n",
    "acc_data = Gesture_Dataset_Accidental(image_train, acc_tensor,transform=transforms.ToTensor())\n",
    "note_loader = torch.utils.data.DataLoader(gest_data, batch_size = btch_sz, shuffle=True, num_workers = 3)\n",
    "acc_loader = torch.utils.data.DataLoader(acc_data, batch_size = btch_sz,shuffle=True, num_workers = 3)\n",
    "cnn_note = CNN_Base_Note_and_Volume(540,720).to(device)\n",
    "cnn_accidental = CNN_Base_Accidental(540,720).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_note = torch.optim.SGD(cnn_note.parameters(), lr=learning_rate) \n",
    "optimizer_acc = torch.optim.SGD(cnn_accidental.parameters(),lr=learning_rate)\n",
    "optimizer_conv_note = torch.optim.SGD(model_conv.parameters(),lr=learning_rate)\n",
    "optimizer_conv_acc = torch.optim.SGD(model_conv_2.parameters(),lr=learning_rate)\n",
    "\n",
    "    \n",
    "#OPTIMIZATION PROCESS\n",
    "\n",
    "\n",
    "for n_e in range(epoch):\n",
    "    for idx, (i,j) in enumerate(note_loader):\n",
    "        print(\"Note: Epoch {}, Test {}\".format(n_e, idx))\n",
    "        i=i.to(device)\n",
    "        #print(i.shape)\n",
    "        j=j.to(device)\n",
    "        #print(j.shape)\n",
    "        print(\"Phase 1\")\n",
    "        op = model_conv(i)\n",
    "        j = j.float()\n",
    "        ls = criterion(op,j)\n",
    "        print(\"Phase 2\")\n",
    "        optimizer_conv_note.zero_grad()\n",
    "        ls.backward()\n",
    "        optimizer_conv_note.step()\n",
    "        print(\"Phase 3\")\n",
    "\n",
    "for n_e in range(epoch):\n",
    "    for idx, (i,j) in enumerate(acc_loader):\n",
    "        print(\"Accidental: Epoch {}, Test {}\".format(n_e, idx))\n",
    "        i=i.to(device)\n",
    "        j=j.to(device)\n",
    "        j = j.float()\n",
    "        op = model_conv_2(i)\n",
    "        ls = criterion(op,j)\n",
    "        optimizer_conv_acc.zero_grad()\n",
    "        ls.backward()\n",
    "        optimizer_conv_acc.step()\n",
    "\n",
    "\n",
    "print(len(note_tensor))\n",
    "print(len(acc_tensor))\n",
    "\n",
    "        \n",
    "torch.save(cnn_note.state_dict(), MODEL_PATH_NOTE)\n",
    "torch.save(cnn_accidental.state_dict(), MODEL_PATH_ACC)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as tdata\n",
    "import os \n",
    "from torch import dtype\n",
    "import pyaudio\n",
    "import skimage\n",
    "import pyaudio\n",
    "from PIL import Image\n",
    "import itertools\n",
    "\n",
    "\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
